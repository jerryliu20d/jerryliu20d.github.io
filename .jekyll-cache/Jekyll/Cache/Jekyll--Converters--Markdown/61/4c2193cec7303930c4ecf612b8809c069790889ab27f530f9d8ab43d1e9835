I"ç!<blockquote>
  <p>Easy model, Good performance</p>
</blockquote>

<h1 id="prerequisite">Prerequisite</h1>

<p>Before you read this post, you should have a have some basic knowledge of kernels and Hilbert space. If not, please read the following materials first: <a href="https://en.wikipedia.org/wiki/Hilbert_space">Hilbert Space</a>, <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space#Moore%E2%80%93Aronszajn_theorem">RKHS</a>, <a href="https://en.wikipedia.org/wiki/Kernel_method">Kernels</a> and <a href="https://en.wikipedia.org/wiki/Support-vector_machine">SVM</a>. I‚Äôll also make some small extension.</p>

<h1 id="overview">Overview</h1>

<p><a href="http://www.nicolasgarciat.com/cv.html">Nicolas Garcia Trillos</a>, Assistant Professort from UW-Madison, wrote summary table to show the connection and difference between SVM and Regularized Linear Regression in both Euclidean space and Hilbert Space. It is super helpful.
<span id="fig1"></span>
<img src="\img\post4-1.jpg" alt="Summary" /></p>

<h1 id="svm-in-euclidean-space">SVM in Euclidean space</h1>

<p>Suppose we have n data points $x_1\dots x_n$ labeled as $y_1\dots y_n$. For linear separable data, we want to perfectly classify the data with hyperplane $H_{\tilde{\beta},\tilde{\beta_0}}$. It‚Äôs fair to let $\Vert\tilde{\beta}\Vert=1$. Here our decision rule is</p>

\[y_i=\left\{
    \begin{array}{lr}
    1~~~~~~~~~~~~&amp;if~\langle\tilde\beta,x_i\rangle+\tilde{\beta_0}&gt;0\\  
    -1~~~~~~~~~~&amp;o.w
    \end{array}
    \right.\]

<p>And we want to minimize the distance between the data point and the hyperplane, i.e. $y_i(‚ü®\tilde{\beta},x_i‚ü©+\tilde{\beta_0})$. Define
$m=\min\limits_{i=1,\dots ,n}y_i(‚ü®\tilde{\beta},x_i‚ü©+\tilde{\beta_0})$. The optimization problem is</p>

\[\max_{\beta,\beta_0}m\\
s.t.~~y_i(‚ü®\beta,x_i‚ü©+\beta_0)\ge m.\]

<p>Now let $\beta = \frac{\tilde{\beta}}{m}$, i.e. $m=\frac{1}{\Vert\beta\Vert}$, because $\Vert\tilde{\beta}\Vert=1$. Then we have</p>

\[\min_{\beta,\beta_0}\frac{\Vert\beta\Vert}{2}\\
s.t.~~y_i(‚ü®\beta,x_i‚ü©+\beta_0)\ge 1.\]

<p>This is the so called hard margin SVM. And Soft Margin SVM is quiet similar. For the dual problem of soft margin SVM, it is stated in the <a href="#fig1">figure</a>. You will find how SVM turn a problem from $\mathcal{R}^p$ to $\mathcal{R}^n$. And the solution to the dual problem is left to you guys. If you are not familiar with dual problem, see <a href="https://drive.google.com/file/d/1ZBAyc1hLMxNPVugfWI0M0gAdZqx29lq0/view?usp=sharing">here</a>.</p>

<h1 id="rhks-and-kernels">RHKS and Kernels</h1>

<p>We define the matrix representation of kernel $k:x*x\longrightarrow\mathcal{R}$ is $K$, which is symmetric and psd. Here we will show RKHS can uniquely define a corresponding kernel and vice versa.</p>

<blockquote>
  <p>RKHS $‚üπ$ Kernel</p>
</blockquote>

<p>We can define RKHS with following:
for every $x\in X$, the map:</p>

\[L_x:f\in H\longrightarrow f(x)\]

<p>is a continuous map. in particular,</p>

\[|f(x)-\tilde{f}(x)|\le C_x\Vert f-\tilde{f}\Vert,~~\forall f,\tilde{f}\in H\]

<p>By <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz representation theorem</a>, there exists a unique $K_x\in H~~s.t.$ <a href="">Why need uniqueness?</a></p>

<p>\(f(x)=L_x(f)=\langle f,k_x\rangle_H,~~\forall f\in H\).</p>

<p>We can also pick $f=k_{\tilde{x}}$, that gives:</p>

\[k_{\tilde{x}}(x)=\langle k_{\tilde{x}},k_{x}\rangle_H\]

<p>Then we define $k(\tilde{x},x)=k_{\tilde{x}}(x)=\langle k_{\tilde{x}},k_{x}\rangle_H$, which is conformable with the kernel $k$. Finally, we define a function $ùúë:X\longrightarrow H$, $s.t.~ùúë(x)=k_x,~\forall x\in X$.</p>

<blockquote>
  <p>Kernel $‚üπ$ RKHS</p>
</blockquote>

<p>The result can be derived from <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">moore aronszajn theorem</a> directly.</p>

<p>We define a Hilbert space:</p>

\[H=\{\sum_{i=1}^\infty a_ik(\cdot,x_i)\}\\
s.t.~\sum_{i=1}^\infty a_i^2k(x_i,x_i)&lt;\infty\]

<p>,where $x_i\in X$, $a_i\in\mathcal{R}$. And the inner product is defined as:</p>

<p>\(\langle\sum_{i=1}^\infty a_ik(\cdot,x_i),\sum_{i=1}^\infty b_jk(\cdot,y_i)\rangle_H‚âê\sum_{i=1}^\infty\sum_{j=1}^\infty a_ib_jk(x_i,x_j)\).</p>

<p>Then we check the reproducing property:</p>

\[\begin{aligned}
\langle f,k(\cdot,x)\rangle_H&amp;=\langle\sum_{i=1}^\infty a_ik(\cdot,x_i),k(\cdot,x)\rangle\\
&amp;=\sum_{i=1}^\infty a_ik(x,x_i)\\
&amp;=f(x)
\end{aligned}\]

<p>To prove uniqueness, let $G$ be another Hilert space with reproducing kernel $k$. FOr any $x$ and $y$ in $X$, we have:</p>

<p>\(\langle k_x,k_y\rangle_H=k(x,y)=\langle k_x,k_y\rangle_G\), by completeness, it is unique.</p>

<h1 id="regularization-in-special-case">Regularization in Special Case</h1>

<p>In linear regression, when the $X$ matrix has the preoblem with illness, we usually use regularization. The idea is</p>

\[\min\limits_{f\in Z}J(f)=\lambda R(f)+\sum_{i=1}^n(f(x_i)-y_i)^2\]

<p>for some family of function $Z$ and regularization $R$.</p>

<blockquote>
  <p>Note that we cannot arbitrary choose one space. For example, we cannot use the $\mathcal{L}^2$ space (Lebesgue spaces), because for the point $x_i$, we cannot make sure $f(x_i)$ makes sense.</p>
</blockquote>

<p>We will choose RKHS and $R(f)=\Vert f\Vert_H^2$ at last, but here let us first choose a special case with $X=[0,1]$.</p>

<p>We define Hilbert space:</p>

\[Z=\left\{
\begin{aligned}
&amp;f:[0,1]\longrightarrow\mathcal{R}\\
&amp;s.t.~f(x)=\int_0^xf'(t)dt
\end{aligned}
\right\}\]

<p>And regularization function $R(f)=\int_0^1(f‚Äô(x))^2dx$.</p>

<hr />
<p>How we solve optimization problem in euclidean space?</p>

<p>We use derivation!</p>

\[\min\limits_{x\in\mathcal{R^m}}F(x)\]

<p>If the optimization point is $x^*$ and we know it is the same problem with</p>

\[\min\limits_{t\in\mathcal{R}}F(x^*+tv)\]

<p>,where the $v$ is constant in $\mathcal{R^m}$. <a href="">If the $x^*$ is not unique?</a></p>

<p>Let $ùúë_v(t)=F(x^*+tv)$, then the minimum reach at $t=0$, i.e.</p>

\[0=ùúë_v'(0)=\langle \nabla F(x^*),v\rangle\]

<blockquote>
  <p>Use $\langle \nabla J(f^*),g\rangle=0$ to solve the optimization problem!</p>
</blockquote>

\[\begin{aligned}
ùúë_g'(t)\Big|_{t=0}=&amp;0\\
\lambda\int_0^1(f^*)'g'dx=&amp;-\sum_{i=1}^n(f^*(x_i)-y_i)g(x_i)
\end{aligned}\]

<p>This is so called Euler-Lagrange Equation. Then we will solve this equation by some tricks.</p>

<ol>
  <li>
    <p>$g=g_1$, where $g_1\doteq min{x,x_1}$</p>

    <p>The E-L equation follows the following equations.</p>

\[\begin{aligned}
 \lambda\int_0^{x_1}(f^*)'(t)dt=&amp;\sum_{i=1}^n(f^*(x_i)-y_i)(x_i\wedge x_1)\\
 \lambda f^*(x_1)-f^*(0)=&amp;\sum_{i=1}^n(f^*(x_i)-y_i)(x_i\wedge x_1)\\
 \lambda f^*(x_1)=&amp;\sum_{i=1}^n(f^*(x_i)-y_i)(x_i\wedge x_1)
 \end{aligned}\]

    <p>Note that the last equation holds because we has already defined that $f(x)=\int_0^xf‚Äô(t)dt$. So $f^*(0)=0$.</p>
  </li>
  <li>
    <p>$g=g_j$, $g_j\doteq min{x,x_j}$, $for~j=1,\dots,n$</p>

    <p>We have the same result:
\(\lambda f^*(x_j)=\sum_{i=1}^n(f^*(x_i)-y_i)(x_i\wedge x_j)\)
<span id="loc1"></span></p>
  </li>
  <li>
    <p>When $x&gt;x_n$</p>

    <p>Because we define the regularization term as $R(f)=\int_0^1(f‚Äô(x))^2dx$. We will just let $f‚Äô(x)=0$. That is a constant.</p>
  </li>
</ol>

<blockquote>
  <p>Actually, it is a piecewise linear function. We will show it by ${(f^*)‚Äô}‚Äô=0$</p>
</blockquote>

<p>Take $g\in Z:[0,1]\longrightarrow\mathcal{R}$ $s.t.~g(0)=0$ and $g(x)=0~~\forall x\ge x-1$.</p>

<p>Then we must have</p>

\[\begin{aligned}
\int_0^{x_1}(f^*)'g'dx&amp;=0\\
-\int_0^{x_1}(f^*)''gdx+(f^*)'g\Big|_0^{x_1}&amp;=0\\
-\int_0^{x_1}(f^*)''gdx&amp;=0\\
(f^*)''g&amp;=0
\end{aligned}\]

<p>This is for arbitrary $g\in Z$ satisfying the previous defined condtion. Thus ${(f^*)‚Äô}‚Äô=0$.</p>

<p>Until now we have proved that $f^*(x)$ in constant on $[0,x_1]$. It is the same for each interval on $[0,x_n]$. And for $x\in[x_n,1]$, we have already showed in <a href="#loc1">part 3</a>.</p>

<h1 id="regularization-in-general-case">Regularization in General Case</h1>

<p>For general case, the optimization problem is:</p>

\[\min_{f\in H}\lambda\Vert f\Vert_H^2+\sum_{i=1}^n(f(x_i)-y_i)^2\]

<p>, where $H$ is RKHS. Let $k:[0,1]*[0,1]\longrightarrow\mathcal{R}$, $k(x,\tilde{x})=min{x,\tilde{x}}$. By the <a href="https://en.wikipedia.org/wiki/Representer_theorem">representor theorem</a>, we have:</p>

\[f^*=\sum_{i=1}^na_i(\cdot\wedge x_i)\]

<p>We find it conincide with the previous result.</p>

<p>#</p>
:ET